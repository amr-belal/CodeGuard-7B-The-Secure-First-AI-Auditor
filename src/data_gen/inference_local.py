from llama_cpp import Llama

# Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ù€ GGUF Ø§Ù„Ù„ÙŠ Ø£Ù†Øª Ù„Ø³Ù‡ Ù…Ù†Ø²Ù„Ù‡
# model_path = r"D:\summer-2026\MyModels\CodeGaurd\Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf"
model_path = "/mnt/d/summer-2026/MyModels/CodeGaurd/Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf"

print("ğŸš€ Loading CodeGuard-7B into RAM...")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
llm = Llama(
    model_path=model_path,
    n_ctx=2048,             # Ø­Ø¬Ù… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù„ÙŠ ÙŠÙ‚Ø¯Ø± ÙŠÙ‚Ø±Ø£Ù‡
    n_gpu_layers=-1,        # Ø§Ø³ØªØ®Ø¯Ù… ÙƒØ§Ø±Øª Ø§Ù„Ø´Ø§Ø´Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ (-1 ÙŠØ¹Ù†ÙŠ ÙƒÙ„Ù‡)
    verbose=False
)

def audit_my_code(user_code):
    # Ø§Ù„Ù€ Prompt Ø§Ù„Ù„ÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§ØªØ¯Ø±Ø¨ Ø¹Ù„ÙŠÙ‡
    prompt = f"System: You are a Security Auditor. Find vulnerabilities and provide a fix.\nUser: {user_code}\nAssistant:"
    
    print("ğŸ” Analyzing security...")
    output = llm(prompt, max_tokens=512, stop=["User:", "\n"], echo=False)
    return output['choices'][0]['text']

if __name__ == "__main__":
    # ÙƒÙˆØ¯ Ø§Ø®ØªØ¨Ø§Ø± ÙÙŠÙ‡ Ø«ØºØ±Ø© SQL Injection
    test_code = "query = 'SELECT * FROM users WHERE id = ' + user_input"
    result = audit_my_code(test_code)
    print("\nğŸ›¡ï¸ CodeGuard Report:\n", result)